# 2.3 Expectation
```table-of-contents
```
---
Although all the probability information of a random variable is contained in its cdf (or pmf for discrete random variables and pdf for continuous random variables), it is often useful to consider various numerical characteristics of that random variable. One such number is the expectation of a random variable; it is a sort of “weighted average” of the values that $X$ can take. Here is a more precise definition.

## Definition 2.4: Expectation of a Discrete Random Variable
Let $X$ be a *discrete* random variable with pmf $f$. The **expectation** or expected value) of $X$, denoted by $\mathbb{E}X$, is defined by
$$
\mathbb{E}X = \sum_{x} x \mathbb{P}(X=x) = \sum_{x} xf(x)
$$
The expectation of $X$ is sometimes written as $\mu_{X}$.

### Example 2.6 (Tossing a Fair Die)
Find $\mathbb{E}X$ if $X$ is the outcome of a toss of a fair die.

Since $\mathbb{P}(X=1)-\dots-\mathbb{P}(X-6)=\frac{1}{6}$, we have
$$
\mathbb{E}X=1\left( \frac{1}{6}  \right) +2\left( \frac{1}{6} \right) +\dots+6\left( \frac{1}{6} \right) = \frac{7}{2}
$$
$\mathbb{E}X$ is not necessarily a possible outcome of the random experiment as in the previous example.

One way to interpret the expectation is as a type of "expected profit". Specifically, suppose we play a game where you throw two dice, and I pay you out, in dollars, the sum of the dice, $X$ say. However, to ender the game you must pay me $d$ do
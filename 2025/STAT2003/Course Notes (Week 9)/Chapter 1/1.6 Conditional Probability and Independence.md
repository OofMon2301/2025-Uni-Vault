# 1.6 Conditional Probability and Independence
How do probabilities change when we know some event $B \subset \Omega$ has occurred? Suppose $B$ has occurred. Thus, we know that the outcome lies in $B$. Then $A$ will occur if and only if $A \cap B$ occurs, and the relative chance of $A$ occurring is therefore
$$
\frac{\mathbb{P}(A \cap B)}{\mathbb{P}(B)}
$$
This leads to the following definition:

**The conditional probability of $A$ given $B$ is:**
$$
\mathbb{P}(A|B) = \frac{\mathbb{P}(A \cap B)}{\mathbb{P}(B)}
$$
### Example 1.10 (Throwing two Dice)
We throw two dice. Given that the sum of the eyes is 10, what is the probability that one 6 is cast?

Let $B$ be the event that the sum is 10,
$$
B = \{ (4,6),(5,5),(6,4) \}.
$$
Let $A$ be the even that one 6 is cast,
$$
A = \{ (1,6),\dots,(5,6),(6,1)\dots,(6,5) \}
$$
Then, $A \cap B = \{ (4,6),(6,4) \}$. And, since all elementary events are equally likely, we have
$$
\mathbb{P}(A|B) = \frac{\left( \frac{2}{36} \right)}{\frac{3}{36}}=\frac{2}{3}
$$
## 1.6.1 Product Rule
By the definition of conditional probability, we have
$$
\mathbb{P}(A \cap B) = \mathbb{P}(A)\mathbb{P}(B|A)
$$
We can generalise this to $n$ intersections $A_{1} \cap A_{2} \cap \dots \cap A_{n}$, which we abbreviate as $A_{1}A_{2}\dots A_{n}$. This gives us the **product rule** of probability (also called *chain rule*.)

### Theorem 1.3 Product Rule
Let $A_{1},\dots,A_{n}$ be a sequence of events with $\mathbb{P}(A_{1},\dots,A_{n-1})>0$. Then,
$$
\mathbb{P}(A_{1}\dots A_{n}) = \mathbb{P}(A_{1})\mathbb{P}(A_{2}|A_{1})\mathbb{P}(A_{3}|A_{1}A_{2})\dots \mathbb{P}(A_{n}|A_{1}\dots A_{n-1}).
$$
*Proof*: We only show the proof for 3 events, since the $n>3$ event case follows similarly. By applying conditional probability to $\mathbb{P}(B|A)$ and $\mathbb{P}(C|A \cap B)$, the left hand side of our product rule is we have:
$$
\mathbb{P}(A)\mathbb{P}(B|A)\mathbb{P}(C|A \cap B) = \mathbb{P}(A) \frac{\mathbb{P}(A \cap B)}{\mathbb{P}(A)} \frac{\mathbb{P}(A \cap B \cap C)}{\mathbb{P}(A \cap B)} = \mathbb{P}(A \cap B \cap C)
$$
Which is equal to the left hand side of the proof above.

## 1.6.2 Law of Total Probability and Bayes' Rule
Suppose $B_{1},B_{2},\dots,B_{n}$ is a partition of $\Omega$. That is, $B_{1},B_{2},\dots,B_{n}$ are disjoint and their union is $\Omega$.

Then, by the sum rule, $\mathbb{P}(A) = \sum_{i=1}^n\mathbb{P}(A \cap B_{i})$ and hence, by the definition of conditional probability, we have
$$
\mathbb{P}(A)=\sum_{i=1}^n \mathbb{P}(A|B_{i})\mathbb{P}(B_{i})
$$
This is called the **law of total probability**.

Combining the Law of Total Probability with the definition of conditional probability gives **Bayes' Rule**:
$$
\mathbb{P}(B_{j}|A) = \frac{\mathbb{P}(A|B_{j})\mathbb{P}(B_{j})}{\sum_{i=1}^n\mathbb{P}(A|B_{i})\mathbb{P}(B_{i})}
$$
## 1.6.3 Independence
Independence is a very important concept in probability and statistics. Loosely speaking it models the *lack of information* between events. We say $A$ and $B$ are *independent* if the knowledge that $A$ has occurred does not change the *probability* that $B$ occurs. That is
$$
A,B \text{ independent} \iff \mathbb{P}(A \cap B) = \mathbb{P}(A)
$$
Since $\mathbb{P}(A|B) = \mathbb{P}(A \cap B)\mathbb{P}(B)$ an alternative definition of independence is:
$$A, B \text{ independent} \iff \mathbb{P}(A \cap B) = \mathbb{P}(A)\mathbb{P}(B)$$
This definition covers the case $B=\emptyset$. We can extend the definition to arbitrarily many events:

### Definition 1.4 - Independent Events
This 